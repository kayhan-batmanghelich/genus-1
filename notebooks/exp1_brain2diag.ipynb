{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1. Predicting diagnosis from sMRI features\n",
    "\n",
    "In this experiment, we try to predict diagnosis group based on features extracted from anatomical MR images using FreeSurfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the Destrieux atlas FreeSurfer statistic outputs and then restricts it to cortical thickness or volume measures. Subcortical volume measures are always included. We include the covariates listed in Table X. For each categortical covariate under the One-hot encoding column, we create a separate feature column for each value of the feature. Each column contains a 1 wherever the feature is present and 0 otherwise. Other covariates are included as is. Redudant columns in the one hot encoded matrix are removed. The effect of the covariates on the sMRI data is removed via a projection matrix. We create the projection matrix $P$ by applying the following transformations:\n",
    "\n",
    "$$P = I - C(C^{T}C)^{-1}C^{T}$$\n",
    "\n",
    "$$X^{*} = PX$$\n",
    "\n",
    "Where $I$ is the identity matrix of the $N x P$ covariate matrix, $C$ is the covariate matrix and $X^{*}$ is the new data matrix with the effect of covariates removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from custom import utils\n",
    "\n",
    "\n",
    "# VALIDATION DATA\n",
    "valbase = '/storage/gablab001/data/genus/current/structured/validate'\n",
    "\n",
    "# 183_cor_subcor_D_thickness.csv contains the 170 features for the validation set\n",
    "valdatacob = '183_cor_subcor_D_thickness.csv'\n",
    "valdatacob = pd.read_csv(os.path.join(valbase, valdatacob))\n",
    "\n",
    "# the file containing the covariates\n",
    "valcov = pd.read_csv(os.path.join(valbase, '183_covariates_to_use.csv'))\n",
    "\n",
    "# this is y in the classification analysis\n",
    "valres = [1 if i == 0 else 0 for i in valcov['diag'].values]\n",
    "\n",
    "valcov = valcov[['ICV','age','gender']]\n",
    "\n",
    "# base directory where some text files live, they incude header names\n",
    "# example: example_header.txt will have:\n",
    "# pallidum\n",
    "# cerebellum\n",
    "txtb = '/storage/gablab001/data/genus/current/structured/genus/text_files_for_indexing'\n",
    "\n",
    "# directory to where the genus brain that is\n",
    "bd = '/storage/gablab001/data/genus/current/structured/brain/'\n",
    "\n",
    "# the header text file for the 170 columns, this will be used to subset the \n",
    "# entire genus data\n",
    "thickness = np.genfromtxt(os.path.join(txtb, '170_columns.txt'), dtype=str)\n",
    "\n",
    "# making equivalent volume headers from the thickness headers\n",
    "volume = ' '.join(thickness).replace('thickness_D','volume_D').split(' ')\n",
    "\n",
    "# this variable is for convenience so that i dont hav to change\n",
    "# where the headers are in multiple places, just here\n",
    "colheads = thickness\n",
    "\n",
    "# loading the covariate headers that will be one hot encoded\n",
    "cvar_encode = np.genfromtxt(os.path.join(txtb, 'covars_ecn.txt'), dtype=str)\n",
    "\n",
    "# the covariate headers that wont be one hot encoded\n",
    "cvar = np.genfromtxt(os.path.join(txtb, 'covars_no_ecn.txt'), dtype=str)\n",
    "\n",
    "# GENUS brain data\n",
    "brain = pd.read_csv(os.path.join(bd, 'GENUS_FS_ATLAS_D.csv'), low_memory=False)\n",
    "\n",
    "# GENUS response variable\n",
    "response = brain[['IID','GROUP']]\n",
    "\n",
    "# here i combined all the needed data so that I can drop rows all together and \n",
    "# make sure all parts of the data, brain regions, covariates, ID, response are\n",
    "# sorted by the same rows\n",
    "combined = pd.concat([\n",
    "    brain[colheads],\n",
    "    brain[cvar],\n",
    "    brain[cvar_encode],\n",
    "    brain[['IID','GROUP']]\n",
    "], axis=1).dropna().drop_duplicates('IID')\n",
    "\n",
    "# the genus y in the classification analysis\n",
    "response = combined['GROUP'].values\n",
    "\n",
    "# subsetting genus data to only include the 170 features\n",
    "X_data = combined[colheads].reset_index(drop=True)\n",
    "\n",
    "# getting the covariates that wont be one hot encoded\n",
    "cvar_ne = combined[cvar].reset_index(drop=True)\n",
    "\n",
    "# and the covariates that will be one hot encoded\n",
    "cvar_e = combined[cvar_encode].reset_index(drop=True)\n",
    "\n",
    "# performing one hot encoding\n",
    "cvar_e = pd.concat([\n",
    "    pd.DataFrame(utils.encoder(cvar_e[col])) for col in cvar_e.columns\n",
    "], axis=1, ignore_index=True)\n",
    "\n",
    "# recombining covariates\n",
    "cvars = pd.concat([cvar_ne, cvar_e], axis=1)\n",
    "\n",
    "# response variable to be fed into classifier\n",
    "y = np.array([1 if i == 'Schizophrenia' else 0 for i in response])\n",
    "\n",
    "# remove effects of covariates\n",
    "# NOTE: this will result in extremely poor results for the validation set\n",
    "#       because the same covariates cannot be applied to the validation set\n",
    "#       it's only when the same covariates are removed from each set that we see\n",
    "#       results similar to what I show\n",
    "non_sing_cvars = utils.make_non_singular(cvars.values)\n",
    "XG = utils.proj(X_data.values, non_sing_cvars)\n",
    "XCO = utils.proj(valdatacob.values, valcov.values)\n",
    "\n",
    "# dictionary that will be passed to the classifier node\n",
    "data_dict = {'GENUS_X':XG, 'GENUS_y':y,\n",
    "             'GENUS_XCOLS':X_data.columns.values,\n",
    "             'GENUS_COVCOLS':cvars.columns.values,\n",
    "             'COBREFMRI_X': XCO, 'COBREFMRI_y': valres,\n",
    "             'COBREFMRI_XCOLS': valdatacob.columns.values,\n",
    "             'COBREFMRI_COVCOLS': valcov.columns.values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "To test how well the brain features predict the diagnosis we employ a nested cross validation scheme. The model of choice is Logistic Regression with and L1 penalty to impose sparcity on the feature space. We picked an L1 penalty because we want to find a subset of the feature space that best predicts the outcome. The best regularization parameter was chosen in the inner loop of the nested cross validation scheme and then it was applied to the other loop. This paramter would determind the features which would be considered in the model at the time of classification. Two cross validators(CV) were chosen, for the outer loop a stratisfied shuffle split was selection, this CV was run over 400 iterations. It randomly sampled from the data in a stratifed manner which preserves the relative perctanges of patients and controls in the entire sample. The inner CV was stratified k-fold, where k = 15. \n",
    "\n",
    "The preprocessing steps before the logistic regression model was applied to the data were demeaning by subtracting the feature mean from each observation in that feature, and scaling by dividing each observation within the feature by the feature standard deviation (after demeaning). The accuracy measured we used was Receiver Operating Characteristic Area Under the Curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_in = StratifiedKFold(n_splits=15, shuffle=True)\n",
    "cv_out = StratifiedShuffleSplit(n_splits=400)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lg', linear_model.LogisticRegressionCV(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        cv=cv_in,\n",
    "        Cs=200,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "clf_rf = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lg', RandomForestClassifier(n_estimators=200))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for idx, (train, test) in enumerate(cv_out.split(data_dict['GENUS_X'], data_dict['GENUS_y'])):\n",
    "    \n",
    "    clf.fit(data_dict['GENUS_X'][train], data_dict['GENUS_y'][train])\n",
    "    \n",
    "    results['coef_{}'.format(idx)] = clf.named_steps['lg'].coef_\n",
    "    \n",
    "    results['test_auc_split_{}'.format(idx)] = roc_auc_score(\n",
    "                                             data_dict['GENUS_y'][test], \n",
    "                                             clf.predict(data_dict['GENUS_X'][test])\n",
    "                                         )\n",
    "    results['train_auc_split_{}'.format(idx)] = roc_auc_score(\n",
    "                                              data_dict['GENUS_y'][train],\n",
    "                                              clf.predict(data_dict['GENUS_X'][train])\n",
    "                                         )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the model used applied an L1 penalty this allowed us to get a view at each of the 400 outer cross validation iterations which features from the input sMRI matrix were included in the model. We aggregated these results and created a stability heat map over the brain. The darker the color of a region, the more times a feature was included in the logistic regression model across the CV 400 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from collections import Counter\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def nonzero(x):\n",
    "    return np.nonzero(x[0])[0]\n",
    "\n",
    "def save_pickle(x, name):\n",
    "    with open(name, \"wb\") as save:\n",
    "        pickle.dump(x, save, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return None\n",
    "\n",
    "test_auc = [val for key, val in results.items() if 'test_auc_split' in key]\n",
    "train_auc = [val for key, val in results.items() if 'train_auc_split' in key]\n",
    "\n",
    "aucdf = pd.DataFrame({'Test_AUC': [val for key, val in results.items() if 'test_auc_split' in key],\n",
    "                      'Train_AUC': [val for key, val in results.items() if 'train_auc_split' in key]})\n",
    "\n",
    "melted = pd.melt(aucdf, var_name='Partition', value_name='ROC AUC SCORE')\n",
    "\n",
    "ancc = [inner for outer in [data_dict['GENUS_XCOLS'][nonzero(results['coef_{}'.format(idx)])]\n",
    "        for idx in range(400)] for inner in outer]\n",
    "\n",
    "region_count = dict(Counter(ancc))\n",
    "\n",
    "lh = {key.replace('lh_', '').replace('.', '-').replace('_thickness_D', ''):val for key, \n",
    "      val in region_count.items() if 'lh' in key}\n",
    "rh = {key.replace('rh_', '').replace('.', '-').replace('_thickness_D', ''):val for key, \n",
    "      val in region_count.items() if 'rh' in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
