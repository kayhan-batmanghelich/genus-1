{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from custom import utils\n",
    "from collections import Counter\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for dealing with the handedness covariate\n",
    "This function performs categorical imputation. \n",
    "    This decision is made based on the fact that ~90 out of 1000+ \n",
    "    handed values are missing. That small percentage is imputed \n",
    "    with the most frequent handedness score \"Right\". There's also\n",
    "    a very small percentage of non right non left values that \n",
    "    are a mix of \"Both\", \"Mixed\", \"Either\", \"Ambidextrous\", these\n",
    "    are all imputed to just \"Both\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_handed(data):\n",
    "    \"\"\"data: pd.DataFrame\"\"\"\n",
    "    counts = Counter(data)\n",
    "    most = max(counts.items())[0]\n",
    "    data = data.copy().fillna(0)\n",
    "    data[data == 0] = 'Right'\n",
    "    both = ['Both','Mixed','Either','Ambidextrous']\n",
    "    for hand in both:\n",
    "        data[data == hand] = 'Both'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing function\n",
    "Inputs are paths to data, sMRI features and SNPs respectively. \"bcvar\" is a list of covariates: ['SEX', 'AGE_MRI', 'EstimatedTotalIntraCranialVol', 'STUDY']. \"brain_cols\" is a numpy array of feature names to subset the sMRI data. \n",
    "First thing that happens is I get a boolen array of where the brain data contains only Controls or Schizophrenics in the Group feature. Both datasets are subsetted by this boolean array, row wise. It's important to note that the datasets loaded are already in the same order rowwise. Then I create 2 sets of covariate matrices and concatenate them into one. The first set contains AGE, SEX, and ICV(EstimatedTotalIntraCranialVol), the second one is a one hot encoded matrix of handedness, the third is a one hot encoded matrix of site(i.e, study). The first step of the analysis does not use the response variable but I safe it to apply stratifiedKFold cross-validation. Two dictionaries are returned, one contains keys and numpy arrays. The name of the keys are required inputs for the first part of the analysis. The dictionary containing the column headers is not required but is saved for later uses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(brain_path, snp_path, bcvar, brain_cols):\n",
    "    \"\"\"brain_path: String, snp_path: String, bcvar: list, \n",
    "    brain_cols: list or np.ndarray. \n",
    "    \"\"\"\n",
    "    # load data\n",
    "    brain_data = pd.read_hdf(brain_path)\n",
    "    snp_data = pd.read_hdf(snp_path)\n",
    "    # get the group status\n",
    "    gr = brain_data.GROUP.values\n",
    "    cnt_scz = np.logical_or(gr == 'Control', gr == 'Schizophrenia')\n",
    "    # subset by indexes cnt_scz\n",
    "    brain_data = brain_data.iloc[cnt_scz, :]\n",
    "    snp_data = snp_data.iloc[cnt_scz, :]\n",
    "    # create set of covariates\n",
    "    icv = 'EstimatedTotalIntraCranialVol'\n",
    "    cov_set1 = pd.DataFrame(\n",
    "        data=np.hstack((snp_data.SEX.values[:, None],\n",
    "                        brain_data.AGE_MRI.values[:, None],\n",
    "                        brain_data[icv].values[:, None])),\n",
    "        columns=['SEX','AGE','EstimatedTotalIntraCranialVol'])\n",
    "    cov_set1 = cov_set1.fillna(0)\n",
    "    cov_set1[cov_set1.AGE == 0] = cov_set1.AGE.mean()\n",
    "    cov_site = utils.make_non_singular(utils.encoder(brain_data.STUDY.values))\n",
    "    cov_site_cols = ['site{}'.format(i) for i in range(cov_site.shape[1])]\n",
    "    cov_site = pd.DataFrame(data=cov_site, columns=cov_site_cols)\n",
    "    cov_hand = utils.encoder(most_handed(brain_data.HANDED))\n",
    "    cov_hand_cols = ['handed{}'.format(i) for i in range(cov_hand.shape[1])]\n",
    "    cov_hand = pd.DataFrame(data=cov_hand, columns=cov_hand_cols)\n",
    "    cvars = pd.concat([cov_set1, cov_site, cov_hand], axis=1)\n",
    "    y = np.array([0 if i == 'Control' else 1 for i in brain_data.GROUP.values])\n",
    "    return {'Z': cvars.values, \n",
    "            'I': brain_data[brain_cols].values, \n",
    "            'G': snp_data.iloc[:, 1:-5].values,\n",
    "            'colnames': snp_data.iloc[:, 1:-5].columns.values,\n",
    "            'y':y}, {'Z_cols':cvars.columns.values,\n",
    "                     'I_cols':brain_cols,\n",
    "                     'G_cols': snp_data.iloc[:, 1:-5].columns.values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "These two functions assist in the analysis. save_preprocessed saves the data to disk that are outputed from the preprocess function above. The paths to where the files are written on disk are returned. This is because I'll be using nipype so to make life easier data isn't passed when interfacing with nipype nodes - just the path to where the data lives. I then load the data using the paths. The cv_maker function creates k-fold stratified cross validation indices and saves them. These indices are used in the matlab script to load the correct subsets of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_preprocessed(preproc_data_dict, preproc_data_dict_col, save_path, dn, cn):\n",
    "    \"\"\"preproc_data_dict: dictionary object returned from\n",
    "    the preprocessing function, (the first - zeroth value of the return) \n",
    "    preproc_data_dict_col: dintionary object returned from\n",
    "    the preprocessing function, (the second - first value of the return)\n",
    "    save_path: string - base path for saving the dictionaries\n",
    "    dn: string - name for saving the data dictionary \n",
    "    cn: string - name for saving the column header dictionary\n",
    "    \"\"\"\n",
    "    save_dict = os.path.join(save_path, dn)\n",
    "    save_cols = os.path.join(save_path, cn)\n",
    "    scipy.io.savemat(save_dict, mdict=preproc_data_dict)\n",
    "    utils.save_pickle(preproc_data_dict_col, save_cols)\n",
    "    return save_dict, save_cols\n",
    "\n",
    "def cv_maker(data_path, save_path):\n",
    "    import scipy.io\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    X = scipy.io.loadmat(data_path)['I']\n",
    "    y = scipy.io.loadmat(data_path)['y'][0]\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=1)\n",
    "    train_idx, test_idx = {}, {}\n",
    "    for idx, (train, test) in enumerate(cv.split(X, y)):\n",
    "        train_idx['train_{}'.format(idx + 1)] = train + 1\n",
    "        test_idx['test_{}'.format(idx + 1)] = test + 1\n",
    "    scipy.io.savemat(save_path, mdict={\"train\":train_idx, \"test\":test_idx})\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "headers_dir = \"/storage/gablab001/data/genus/fs_cog/pred_diag/column_headers\"\n",
    "brain_cols = np.genfromtxt(os.path.join(headers_dir, \"XB\"), dtype=str)\n",
    "brain_path = \"brain_N1547_P5927_matched.hdf5py\"\n",
    "snp_path = \"genomic_N1547_P100006_matched.hdf5py\"\n",
    "bcv = ['SEX', 'AGE_MRI', 'EstimatedTotalIntraCranialVol', 'STUDY']\n",
    "all_data, all_cols = preprocess(brain_path, snp_path, bcv, brain_cols)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype import Function, Node, Workflow, IdentityInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian analysis - \"step 1\"\n",
    "Below I create the workflow that I use with nipype, create the nipype wrapper nodes to wrap functions that will go into the nipype graph, and then submit the jobs. Due to the nature of the analysis we are parallelizing over the feature space in the sMRI data. That is - one job per feature, on top of that we are parallelizing the cross validation step. In total this means there are (170*10) jobs that need to be submitted. For a single user in my experience that's too many jobs for the Openmind cluster so I limit the amount of jobs that can be submitted at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV_maker = Node(interface=Function(\n",
    "    input_names = ['data_path', 'save_path'],\n",
    "    output_names = ['save_path'],\n",
    "    function = cv_maker\n",
    "), name = 'CV_maker')\n",
    "\n",
    "#CV_maker.inputs.data_path = \"/storage/gablab001/data/genus/brain_genomic_bayes/brain_gene.mat\"\n",
    "#CV_maker.inputs.save_path = \"/storage/gablab001/data/genus/brain_genomic_bayes/cv_idxs.mat\"\n",
    "\n",
    "#cv_maker(\"/storage/gablab001/data/genus/brain_genomic_bayes/brain_gene.mat\",\n",
    "#         \"/storage/gablab001/data/genus/brain_genomic_bayes/cv_idxs.mat\")\n",
    "\n",
    "wf = Workflow(name='brain_bcv')\n",
    "wf.base_dir = \"/om/user/ysa\"\n",
    "\n",
    "Iternode = Node(IdentityInterface(fields=['col_idx', 'cv_idx']), name = 'Iternode')\n",
    "Iternode.iterables = [('col_idx', np.arange(170) + 1), ('cv_idx', np.arange(10) + 1)]\n",
    "\n",
    "def run_bayes(in_file, cv_file, cv_idx, col_idx, out_file):\n",
    "    import cPickle as pickle\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import nipype.interfaces.matlab as Matlab\n",
    "    def outnames(col, out):\n",
    "        return os.path.join(out, '{}.mat'.format(col))\n",
    "    col_names = np.genfromtxt(\"/storage/gablab001/data/genus/brain_genomic_bayes/170_columns.txt\", dtype=str)\n",
    "    col_save_name = col_names[col_idx - 1] + \"_{}_{}_BF\".format(cv_idx, col_idx)\n",
    "    with open(\"/storage/gablab001/data/genus/brain_genomic_bayes/bayes_reg.m\", \"r\") as src:\n",
    "        script = src.read().replace(\"\\n\", \"\")\n",
    "    mat_file = outnames(in_file[:-4] + col_save_name, out_file)\n",
    "    matlab = Matlab.MatlabCommand()\n",
    "    matlab.inputs.paths = [\n",
    "    '/storage/gablab001/data/genus/current/variational_bayes_wrap/varbvs/varbvs-MATLAB',\n",
    "    '/storage/gablab001/data/genus/current/variational_bayes_wrap/varbvs',\n",
    "    '/storage/gablab001/data/genus/current/variational_bayes_wrap/varbvs/varbvs-R']\n",
    "    matlab.inputs.script = script.format(in_file, cv_file, cv_idx, col_idx, mat_file)\n",
    "    res = matlab.run()\n",
    "    return mat_file\n",
    "\n",
    "Run_bayes = Node(interface=Function(\n",
    "    input_names = ['in_file', 'cv_file','cv_idx',\n",
    "                   'col_idx','out_file'],\n",
    "    output_names = ['mat_file'],\n",
    "    function = run_bayes\n",
    "), name='Run_bayes')\n",
    "\n",
    "Run_bayes.inputs.in_file = \"/storage/gablab001/data/genus/brain_genomic_bayes/brain_gene.mat\"\n",
    "Run_bayes.inputs.cv_file = \"/storage/gablab001/data/genus/brain_genomic_bayes/cv_idxs.mat\"\n",
    "Run_bayes.inputs.out_file = \"/storage/gablab001/data/genus/brain_genomic_bayes/\"\n",
    "\n",
    "wf.connect(Iternode, 'cv_idx', Run_bayes, 'cv_idx')\n",
    "wf.connect(Iternode, 'col_idx', Run_bayes, 'col_idx')\n",
    "\n",
    "wf.run(plugin='SLURM', plugin_args={'sbatch_args':'--mem=2G -t 05:00:00', 'max_jobs': 50})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
